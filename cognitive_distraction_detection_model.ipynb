{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the EEG data has already been preprocessed in matlab using EEGlab\n",
    "This project aims to detect cognitive distraction in drivers using their EEG data. Due to privacy concerns, the data cannot be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyEDFlib pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data combining and labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyedflib\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def edf_to_csv(edf_file_path, csv_file_path):\n",
    "    # Open the EDF file\n",
    "    f = pyedflib.EdfReader(edf_file_path)\n",
    "    \n",
    "    # Extract signal labels\n",
    "    signal_labels = f.getSignalLabels()\n",
    "    \n",
    "    # Initialize a dictionary to store the data\n",
    "    data_dict = {label: [] for label in signal_labels}\n",
    "    \n",
    "    # Extract the data for each signal\n",
    "    for i in range(f.signals_in_file):\n",
    "        data_dict[signal_labels[i]] = f.readSignal(i)\n",
    "    \n",
    "    # Close the EDF file\n",
    "    f.close()\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data_dict)\n",
    "    \n",
    "    # Save the DataFrame as a CSV file\n",
    "    df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "\n",
    "# Specify the directory containing the EDF files\n",
    "#edf_directory = r\"C:\\Users\\DELL\\Documents\\AllStudentsRecords\\clean\\name\"\n",
    "\n",
    "# Specify the directory where the CSV files should be saved\n",
    "csv_directory = r\"C:\\Users\\DELL\\Documents\\AllStudentsRecords\\clean\\data\"\n",
    "\n",
    "\n",
    "# Iterate over all EDF files in the specified directory\n",
    "for filename in os.listdir(edf_directory):\n",
    "    if filename.endswith(\".edf\"):\n",
    "        edf_file_path = os.path.join(edf_directory, filename)\n",
    "        csv_file_path = os.path.join(csv_directory, os.path.splitext(filename)[0] + \".csv\")\n",
    "        \n",
    "        # Convert the EDF file to CSV\n",
    "        edf_to_csv(edf_file_path, csv_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label_to_csv(csv_file_path, labeled_csv_path):\n",
    "    # Extract the label from the file name \n",
    "    filename = os.path.basename(csv_file_path)\n",
    "    label = filename.split('_')[-1].split('.')[0] \n",
    "    \n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Add a new column 'Label' with the extracted label\n",
    "    df['Label'] = label\n",
    "    \n",
    "    # Save the labeled DataFrame to the new CSV file\n",
    "    df.to_csv(labeled_csv_path, index=False)\n",
    "\n",
    "\n",
    "#directory containing the original CSV files\n",
    "csv_directory = r\"C:\\Users\\DELL\\Documents\\AllStudentsRecords\\clean\\data\"\n",
    "\n",
    "#directory where the labeled CSV files should be saved\n",
    "labeled_csv_directory = r\"C:\\Users\\DELL\\Documents\\AllStudentsRecords\\clean\\labeled\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all CSV files in the specified directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        csv_file_path = os.path.join(csv_directory, filename)\n",
    "        labeled_csv_path = os.path.join(labeled_csv_directory, filename)\n",
    "        add_label_to_csv(csv_file_path, labeled_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def remove_status_and_combine_csv_files(directory, output_file):\n",
    "    # List to hold DataFrames\n",
    "    dfs = []\n",
    "    \n",
    "    # Iterate over all CSV files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            # Read each CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Remove the 'Status' column if it exists\n",
    "            if 'Status' in df.columns:\n",
    "                df = df.drop(columns=['Status'])\n",
    "            \n",
    "            # Append the modified DataFrame to the list\n",
    "            dfs.append(df)\n",
    "    \n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    # Save the combined DataFrame to a single CSV file\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the directory containing the labeled CSV files\n",
    "labeled_csv_directory = r\"C:\\Users\\DELL\\Documents\\AllStudentsRecords\\clean\\labeled\"\n",
    "\n",
    "# Specify the path for the output combined CSV file\n",
    "output_file = r\"C:\\Users\\DELL\\Documents\\AllStudentsRecords\\clean\\combined_labeled_data.csv\"\n",
    "\n",
    "# Combine all the labeled CSV files into one and remove 'Status' column\n",
    "remove_status_and_combine_csv_files(labeled_csv_directory, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the Data and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(output_file)\n",
    "\n",
    "#set parametrs \n",
    "window_size = 100  \n",
    "sampling_rate = 128  #I set this value according to the sampling rate of the Emotiv device\n",
    "\n",
    "#lists to store features and labels\n",
    "all_features = []\n",
    "all_labels = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function to extract time-domain and frequency-domain features from a each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy.signal import welch\n",
    "\n",
    "def extract_features_from_channel(segment, sf):\n",
    "    features = []\n",
    "    \n",
    "    # Time-Domain Features\n",
    "    features.append(np.mean(segment))                 # Mean\n",
    "    features.append(np.std(segment))                  # Standard Deviation\n",
    "    features.append(np.var(segment))                  # Variance\n",
    "    features.append(skew(segment))                    # Skewness\n",
    "    features.append(kurtosis(segment))                # Kurtosis\n",
    "    features.append(np.max(segment) - np.min(segment))  # Peak-to-Peak Amplitude\n",
    "    \n",
    "    # Frequency-Domain Features\n",
    "    f, Pxx = welch(segment, fs=sf)  # Compute Power Spectral Density (PSD)\n",
    "    \n",
    "    # Band powers\n",
    "    delta_power = np.sum(Pxx[(f >= 0.5) & (f < 4)])\n",
    "    theta_power = np.sum(Pxx[(f >= 4) & (f < 8)])\n",
    "    alpha_power = np.sum(Pxx[(f >= 8) & (f < 13)])\n",
    "    beta_power = np.sum(Pxx[(f >= 13) & (f < 30)])\n",
    "    gamma_power = np.sum(Pxx[(f >= 30) & (f <= 45)])\n",
    "    \n",
    "    features.extend([delta_power, theta_power, alpha_power, beta_power, gamma_power])\n",
    "    \n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To address the issue of a small dataset size, the data is divided into smaller segments. This segmentation helps in managing and processing the data more effectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\scipy\\signal\\_spectral_py.py:600: UserWarning: nperseg = 256 is greater than input length  = 100, using nperseg = 100\n",
      "  freqs, _, Pxy = _spectral_helper(x, y, fs, window, nperseg, noverlap,\n"
     ]
    }
   ],
   "source": [
    "# Group data by labels to ensure segments don't mix labels\n",
    "grouped_data = data.groupby('Label')\n",
    "\n",
    "# Loop over each group to create segments and extract features\n",
    "for label, group in grouped_data:\n",
    "    # Loop through the group in windows\n",
    "    for start in range(0, len(group) - window_size + 1, window_size):\n",
    "        segment = group.iloc[start:start + window_size]  # Define the segment\n",
    "        \n",
    "        segment_features = []\n",
    "        \n",
    "        for channel in group.columns[:-1]:  # Exclude the 'Label' column\n",
    "            channel_data = segment[channel].values  # Get the segment data for this channel\n",
    "            channel_features = extract_features_from_channel(channel_data, sampling_rate)\n",
    "            segment_features.extend(channel_features)  \n",
    "        \n",
    "        all_features.append(segment_features)\n",
    "        \n",
    "        # Assign the label to this segment\n",
    "        all_labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Convert features and labels to DataFrame and Series\n",
    "X_features = pd.DataFrame(all_features)\n",
    "y = pd.Series(all_labels)\n",
    "\n",
    "# Encode the labels into integers\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalize the feature data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "# Reshape X to be 3D [samples, time steps, features] as required by GRU\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Convert labels to categorical (one-hot encoding) if it's a classification problem\n",
    "y_categorical = to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_reshaped, y_categorical, test_size=0.2, random_state=42, stratify=y_encoded )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples:  909\n",
      "number of time steps per sample:  1\n",
      "number of features per time step:  352\n"
     ]
    }
   ],
   "source": [
    "print('number of samples: ', X_train.shape[0])\n",
    "print('number of time steps per sample: ', X_train.shape[1])\n",
    "print('number of features per time step: ', X_train.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">136,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">60,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">10,100</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">303</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ gru_4 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │       \u001b[38;5;34m136,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m100\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru_5 (\u001b[38;5;33mGRU\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m60,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │        \u001b[38;5;34m10,100\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m303\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,203</span> (809.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m207,203\u001b[0m (809.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">207,203</span> (809.39 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m207,203\u001b[0m (809.39 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense, Dropout\n",
    "\n",
    "\n",
    "\n",
    "# Build the GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GRU(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(y_categorical.shape[1], activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "29/29 - 3s - 114ms/step - accuracy: 0.5699 - loss: 0.9468 - val_accuracy: 0.7149 - val_loss: 0.7157\n",
      "Epoch 2/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.7844 - loss: 0.5399 - val_accuracy: 0.8421 - val_loss: 0.3948\n",
      "Epoch 3/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.8669 - loss: 0.3226 - val_accuracy: 0.8509 - val_loss: 0.3043\n",
      "Epoch 4/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.9186 - loss: 0.2027 - val_accuracy: 0.8816 - val_loss: 0.2665\n",
      "Epoch 5/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.9571 - loss: 0.1323 - val_accuracy: 0.8860 - val_loss: 0.2542\n",
      "Epoch 6/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.9648 - loss: 0.1013 - val_accuracy: 0.9035 - val_loss: 0.2393\n",
      "Epoch 7/12\n",
      "29/29 - 0s - 7ms/step - accuracy: 0.9615 - loss: 0.0877 - val_accuracy: 0.9298 - val_loss: 0.2262\n",
      "Epoch 8/12\n",
      "29/29 - 0s - 5ms/step - accuracy: 0.9791 - loss: 0.0582 - val_accuracy: 0.9167 - val_loss: 0.2264\n",
      "Epoch 9/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.9802 - loss: 0.0523 - val_accuracy: 0.9079 - val_loss: 0.2258\n",
      "Epoch 10/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.9923 - loss: 0.0285 - val_accuracy: 0.9386 - val_loss: 0.1815\n",
      "Epoch 11/12\n",
      "29/29 - 0s - 7ms/step - accuracy: 0.9923 - loss: 0.0219 - val_accuracy: 0.9211 - val_loss: 0.1954\n",
      "Epoch 12/12\n",
      "29/29 - 0s - 6ms/step - accuracy: 0.9890 - loss: 0.0364 - val_accuracy: 0.9254 - val_loss: 0.2124\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=12, batch_size=32, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 - 0s - 6ms/step - accuracy: 0.9254 - loss: 0.2124\n",
      "Test Accuracy:  0.9254385828971863\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=2)\n",
    "print('Test Accuracy: ', accuracy )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step\n",
      "classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        chat       0.97      0.99      0.98        72\n",
      "        math       0.91      0.88      0.90        73\n",
      "      nochat       0.89      0.92      0.90        83\n",
      "\n",
      "    accuracy                           0.93       228\n",
      "   macro avg       0.93      0.93      0.93       228\n",
      "weighted avg       0.93      0.93      0.93       228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "\n",
    "report = classification_report(y_true, y_pred_classes, target_names=label_encoder.classes_)\n",
    "print(\"classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
